{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('all')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data \n",
    "barbie_data = pd.read_csv(\"barbie_Cleaned.csv\")\n",
    "barbie_data = barbie_data[barbie_data['rating'] != '1']\n",
    "barbie_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean text\n",
    "#separate conjoined words using wordninja \n",
    "\n",
    "import wordninja\n",
    "barbie_data['text'] = barbie_data['text'].apply(lambda x: ' '.join(wordninja.split(x)))\n",
    "barbie_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create date column\n",
    "barbie_data = pd.DataFrame(barbie_data)\n",
    "\n",
    "# Extract date using regular expression\n",
    "date_pattern = r'(\\d{1,2})\\s(July)\\s(\\d{4})'\n",
    "barbie_data['full_date'] = barbie_data['text'].str.extract(date_pattern).apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "# Display DataFrame with new column\n",
    "print(barbie_data['full_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove non-english words\n",
    "\n",
    "with open('words.txt', 'r') as file:\n",
    "    english_words = set(word.strip().lower() for word in file)\n",
    "\n",
    "# Define a function to remove non-English words\n",
    "def remove_non_english_words(text):\n",
    "    words = text.split()\n",
    "    valid_words = [word for word in words if word.lower() in english_words]\n",
    "    return ' '.join(valid_words)\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "barbie_data['text'] = barbie_data['text'].apply(remove_non_english_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove numbers\n",
    "import re\n",
    "barbie_data['text'] = barbie_data['text'].apply(lambda x: re.sub(r'\\d+', '', x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove phrases\n",
    "phrases_to_remove = ['found', 'helpful', 'review', 'Sign','vote', 'Permalink', 'Warning', 'Spoilers', 'July']\n",
    "barbie_data['text'] = barbie_data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in phrases_to_remove]) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation \n",
    "import string\n",
    "barbie_data['text'] = barbie_data['text'].apply(lambda x: x.translate(str.maketrans(\"\", \"\", string.punctuation)) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove letters that aren't in words\n",
    "def remove_non_word_letters(text):\n",
    "    words = wordninja.split(text)\n",
    "    valid_words = [word for word in words if word.isalpha()]\n",
    "    return ' '.join(valid_words)\n",
    "\n",
    "# Apply the function to the 'text' column\n",
    "barbie_data['text'] = barbie_data['text'].apply(remove_non_word_letters)\n",
    "barbie_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert rating to numeric\n",
    "barbie_data['rating'] = pd.to_numeric(barbie_data['rating'], errors = 'coerce')\n",
    "\n",
    "#drop Nas\n",
    "barbie_data = barbie_data.dropna(subset = ['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert barbie_data to csv\n",
    "barbie_data.to_csv('barbie_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##distribution of review lengths by ratings \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.DataFrame(barbie_data)\n",
    "\n",
    "#compute length of each review\n",
    "df['review_length'] = df['text'].apply(len)\n",
    "\n",
    "# establish color for each rating \n",
    "colors = {1: 'darksalmon', 2: 'firebrick', 3: 'darkorange', 4: 'gold', 5: 'lemonchiffon', 6: 'darkseagreen', 7:'seagreen', 8: 'lightblue', 9: 'steelblue'}\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "#create seperate histogram for each rating subsetted by review length  \n",
    "for rating in sorted(df['rating'].unique()):\n",
    "    subset = df[df['rating'] == rating]\n",
    "    plt.hist(subset['review_length'], bins=20, alpha=0.5, label=f'Rating {rating}', color=colors[rating])\n",
    "\n",
    "plt.xlabel('Review Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Review Lengths by Ratings')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Distribution of review lengths\n",
    "ratings_data = barbie_data['rating']\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df = pd.DataFrame({'ratings': ratings_data})\n",
    "\n",
    "# Create a histogram using Seaborn\n",
    "sns.set(style=\"whitegrid\")  # Set the style\n",
    "sns.histplot(df['ratings'], kde=True, bins=10, color='skyblue')  # Create the distribution plot\n",
    "plt.title('Distribution Plot of Ratings')  # Add title\n",
    "plt.xlabel('Ratings')  # Add x-axis label\n",
    "plt.ylabel('Frequency')  # Add y-axis label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plot top 40 most used adjectives in reviews \n",
    "\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "reviews = barbie_data['text']\n",
    "\n",
    "#remove adjectives that aren't helpful for analysis\n",
    "words_to_remove = ['other', 'sure', 'first', 'many', 'same', 'own', 'little', 'most', 'few', 'main', 'such']\n",
    "\n",
    "# Function to extract adjectives from a spacy parsed document\n",
    "def extract_adjectives(doc):\n",
    "    adjectives = [token.text for token in doc if token.pos_ == 'ADJ' and token.text.lower() not in words_to_remove]\n",
    "    return adjectives\n",
    "\n",
    "# Tokenize each review, parse them using spacy, and extract adjectives\n",
    "adjectives = []\n",
    "for review in reviews:\n",
    "    doc = nlp(review)\n",
    "    adjectives.extend(extract_adjectives(doc))\n",
    "\n",
    "# Count the frequency of each adjective\n",
    "adjective_freq = Counter(adjectives)\n",
    "\n",
    "top_40_adjectives = dict(adjective_freq.most_common(40))\n",
    "\n",
    "# Plotting the frequency distribution for the top 20 adjectives\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_40_adjectives.keys(), top_40_adjectives.values(), color='skyblue')\n",
    "plt.xlabel('Adjectives')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 40 Adjectives in Reviews')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get vader scores \n",
    "import nltk\n",
    "nltk.data.path.append('/Users/maryellenschuster/nltk_data')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "barbie_data['vader_scores'] = barbie_data['text'].apply(lambda x: SentimentIntensityAnalyzer().polarity_scores(x)['compound'])\n",
    "barbie_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert vader scores to numeric \n",
    "barbie_data['vader_scores'] = pd.to_numeric(barbie_data['vader_scores'])\n",
    "barbie_data['rating'] = pd.to_numeric(barbie_data['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot sentiment scores overtime\n",
    "\n",
    "df = pd.DataFrame(barbie_data)\n",
    "df['full_date'] = pd.to_datetime(df['full_date'])  # Convert date column to datetime format\n",
    "\n",
    "# Plot sentiment scores over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['full_date'], df['vader_scores'], marker='o', color='b', linestyle='-')\n",
    "\n",
    "plt.title('Sentiment Scores Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sentiment Score')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution of sentiment scores \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sentiment_scores = barbie_data['vader_scores']\n",
    "\n",
    "# Plotting the distribution of sentiment scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(sentiment_scores, bins=10, color='skyblue', edgecolor='black')  # Adjust the number of bins as needed\n",
    "plt.title('Distribution of Sentiment Scores')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any missing values and handle them if necessary\n",
    "barbie_data.dropna(inplace=True)\n",
    "\n",
    "# Create a scatter plot to visualize the relationship\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='vader_scores', y='rating', data=barbie_data)\n",
    "plt.title('Correlation between VADER Sentiment Scores and Ratings')\n",
    "plt.xlabel('VADER Sentiment Score')\n",
    "plt.ylabel('Rating')\n",
    "plt.grid(True)\n",
    "\n",
    "# Calculate the correlation coefficient\n",
    "correlation_coefficient = barbie_data['vader_scores'].corr(barbie_data['rating'])\n",
    "print(f\"Correlation Coefficient: {correlation_coefficient}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = barbie_data['vader_scores']\n",
    "y = barbie_data['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values.reshape(-1, 1)\n",
    "X_test = X_test.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression model \n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate predicted values\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate evaluation statistics\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(f'Mean Squared Error: {mse:.2f}')\n",
    "print(f'R-squared: {r2:.2f}')\n",
    "\n",
    "barbie_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Define a function to map sentiment scores to predicted ratings\n",
    "def map_vader_to_rating(vader_score):\n",
    "    if vader_score >= 0.5:\n",
    "        return 5  # High positive sentiment\n",
    "    elif vader_score >= 0:\n",
    "        return 4  # Low positive sentiment\n",
    "    elif vader_score >= -0.5:\n",
    "        return 3  # Low negative sentiment\n",
    "    else:\n",
    "        return 2  # High negative sentiment\n",
    "\n",
    "# Apply the function to create the new column\n",
    "barbie_data['predicted_ratings'] = barbie_data['vader_scores'].apply(map_vader_to_rating)\n",
    "\n",
    "print(barbie_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
